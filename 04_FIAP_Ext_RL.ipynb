{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwMsZhho2jwYk9H+8U8JXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcbjrrr/quantai/blob/main/04_FIAP_Ext_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning (Gym Lib)"
      ],
      "metadata": {
        "id": "34tntoBYpi9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gym Library"
      ],
      "metadata": {
        "id": "m5HAvwGtlxm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gym library (now maintained as Gymnasium) is a Python toolkit designed to help you build and interact with environments for reinforcement learning. It provides a standard way to create environments where an AI agent can observe the state of the world, take actions, and receive feedback in the form of rewards. This makes it easier to design, test, and train agents without worrying about the details of the environment itself. For example, you can use Gym to train an agent to balance a pole, play simple video games, or solve optimization problems, all within a consistent and easy-to-use framework. It's like a sandbox where your agent learns by trial and error through interaction."
      ],
      "metadata": {
        "id": "1nNEcGOtlxqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class Environment(gym.Env):\n",
        "    def __init__(self, max_steps=33):\n",
        "        super().__init__()\n",
        "        self.max_steps = max_steps\n",
        "        self.current_step = 0\n",
        "        # Actions: 0: C+, 1: C-, 2: L+, 3: L-\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.observation_space = spaces.Box(low=0, high=0)\n",
        "        self.C = 0\n",
        "        self.L = 0\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return np.array([self.C, self.L])\n",
        "\n",
        "    def _is_valid(self, C, L):\n",
        "        return (2 * C + L <= 10) and (3 * C + L <= 12) and (C >= 0 and L >= 0)\n",
        "\n",
        "    def _calc_reward(self, C, L):\n",
        "        if self._is_valid(C, L):\n",
        "            return 1.5 * C + 1.0 * L\n",
        "        else:\n",
        "            return -10  # heavy penalty for violating constraints\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "\n",
        "        prev_C, prev_L = self.C, self.L\n",
        "\n",
        "        if action == 0:  # C+\n",
        "            self.C += 1\n",
        "        elif action == 1 and self.C > 0:  # C-\n",
        "            self.C -= 1\n",
        "        elif action == 2:  # L+\n",
        "            self.L += 1\n",
        "        elif action == 3 and self.L > 0:  # L-\n",
        "            self.L -= 1\n",
        "\n",
        "        # Ensure C and L do not go below zero, although handled by C-/L- checks\n",
        "        #self.C = max(0, self.C)\n",
        "        #self.L = max(0, self.L)\n",
        "\n",
        "        reward = self._calc_reward(self.C, self.L)\n",
        "        observation = self._get_obs()\n",
        "        terminated = self.current_step >= self.max_steps\n",
        "        return observation, reward, terminated\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.C = 0\n",
        "        self.L = 0\n",
        "        self.current_step = 0\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = {\"current_C\": self.C, \"current_L\": self.L}\n",
        "        return observation, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        # For this problem, a simple print statement is sufficient for \"human\" mode\n",
        "        if mode == 'human':\n",
        "            print(f\"Current State: C={self.C}, L={self.L}\")\n",
        "\n",
        "    def close(self):\n",
        "        # No resources to close for this simple environment\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomAgent:\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def choose_action(self):\n",
        "        return self.action_space.sample() # Randomly sample from the action space\n",
        "\n"
      ],
      "metadata": {
        "id": "LqA7FX0blyv-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env = Environment(max_steps=33)\n",
        "agent = RandomAgent(env.action_space)\n",
        "\n",
        "best_reward = 0\n",
        "best_state = (0, 0)\n",
        "best_step_num = -1\n",
        "\n",
        "observation, info = env.reset()\n",
        "done = False\n",
        "current_step_num = 0\n",
        "\n",
        "while not done:\n",
        "    action = agent.choose_action()\n",
        "    next_observation, reward, done = env.step(action)\n",
        "\n",
        "    current_C, current_L = next_observation[0], next_observation[1]\n",
        "\n",
        "    print(f\"Step {current_step_num}: State={observation}, Action={action}, Next ={(current_C, current_L)}, Reward={reward}\")\n",
        "\n",
        "    if env._is_valid(current_C, current_L) and reward > best_reward:\n",
        "        best_reward = reward\n",
        "        best_state = (current_C, current_L)\n",
        "        best_step_num = current_step_num\n",
        "\n",
        "    observation = next_observation\n",
        "    current_step_num += 1\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"\\nðŸ§  Best valid solution found:\")\n",
        "print(f\"  C = {best_state[0]}, L = {best_state[1]}, P = {best_reward} (found at step {best_step_num})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBF8ZaOcpLem",
        "outputId": "9f7e0439-b752-4fcb-f10f-9260b7809784"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: State=[0 0], Action=2, Next =(np.int64(0), np.int64(1)), Reward=1.0\n",
            "Step 1: State=[0 1], Action=3, Next =(np.int64(0), np.int64(0)), Reward=0.0\n",
            "Step 2: State=[0 0], Action=2, Next =(np.int64(0), np.int64(1)), Reward=1.0\n",
            "Step 3: State=[0 1], Action=1, Next =(np.int64(0), np.int64(1)), Reward=1.0\n",
            "Step 4: State=[0 1], Action=0, Next =(np.int64(1), np.int64(1)), Reward=2.5\n",
            "Step 5: State=[1 1], Action=0, Next =(np.int64(2), np.int64(1)), Reward=4.0\n",
            "Step 6: State=[2 1], Action=0, Next =(np.int64(3), np.int64(1)), Reward=5.5\n",
            "Step 7: State=[3 1], Action=0, Next =(np.int64(4), np.int64(1)), Reward=-10\n",
            "Step 8: State=[4 1], Action=0, Next =(np.int64(5), np.int64(1)), Reward=-10\n",
            "Step 9: State=[5 1], Action=0, Next =(np.int64(6), np.int64(1)), Reward=-10\n",
            "Step 10: State=[6 1], Action=3, Next =(np.int64(6), np.int64(0)), Reward=-10\n",
            "Step 11: State=[6 0], Action=2, Next =(np.int64(6), np.int64(1)), Reward=-10\n",
            "Step 12: State=[6 1], Action=2, Next =(np.int64(6), np.int64(2)), Reward=-10\n",
            "Step 13: State=[6 2], Action=2, Next =(np.int64(6), np.int64(3)), Reward=-10\n",
            "Step 14: State=[6 3], Action=3, Next =(np.int64(6), np.int64(2)), Reward=-10\n",
            "Step 15: State=[6 2], Action=1, Next =(np.int64(5), np.int64(2)), Reward=-10\n",
            "Step 16: State=[5 2], Action=0, Next =(np.int64(6), np.int64(2)), Reward=-10\n",
            "Step 17: State=[6 2], Action=0, Next =(np.int64(7), np.int64(2)), Reward=-10\n",
            "Step 18: State=[7 2], Action=2, Next =(np.int64(7), np.int64(3)), Reward=-10\n",
            "Step 19: State=[7 3], Action=2, Next =(np.int64(7), np.int64(4)), Reward=-10\n",
            "Step 20: State=[7 4], Action=2, Next =(np.int64(7), np.int64(5)), Reward=-10\n",
            "Step 21: State=[7 5], Action=0, Next =(np.int64(8), np.int64(5)), Reward=-10\n",
            "Step 22: State=[8 5], Action=3, Next =(np.int64(8), np.int64(4)), Reward=-10\n",
            "Step 23: State=[8 4], Action=2, Next =(np.int64(8), np.int64(5)), Reward=-10\n",
            "Step 24: State=[8 5], Action=0, Next =(np.int64(9), np.int64(5)), Reward=-10\n",
            "Step 25: State=[9 5], Action=2, Next =(np.int64(9), np.int64(6)), Reward=-10\n",
            "Step 26: State=[9 6], Action=0, Next =(np.int64(10), np.int64(6)), Reward=-10\n",
            "Step 27: State=[10  6], Action=1, Next =(np.int64(9), np.int64(6)), Reward=-10\n",
            "Step 28: State=[9 6], Action=1, Next =(np.int64(8), np.int64(6)), Reward=-10\n",
            "Step 29: State=[8 6], Action=2, Next =(np.int64(8), np.int64(7)), Reward=-10\n",
            "Step 30: State=[8 7], Action=2, Next =(np.int64(8), np.int64(8)), Reward=-10\n",
            "Step 31: State=[8 8], Action=0, Next =(np.int64(9), np.int64(8)), Reward=-10\n",
            "Step 32: State=[9 8], Action=1, Next =(np.int64(8), np.int64(8)), Reward=-10\n",
            "\n",
            "ðŸ§  Best valid solution found:\n",
            "  C = 3, L = 1, P = 5.5 (found at step 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ativity: Gym Lib"
      ],
      "metadata": {
        "id": "nVEbyMdjpqBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to revisit the problem you previously solved and re-implement your solution using OpenAIâ€™s Gym library, which provides a standardized suite of environments for developing and testing reinforcement learning algorithms. Build your agent within the chosen environment and evaluate its performance using relevant metrics such as cumulative reward or the number of episodes required to achieve stable behavior. Finally, compare the results of your Gym-based implementation with your earlier approach."
      ],
      "metadata": {
        "id": "TBFRdfW9prry"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HEXydizHqXWa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}