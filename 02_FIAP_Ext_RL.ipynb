{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDhAPsOHy/9ZjaRw/5MeUh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcbjrrr/quantai/blob/main/02_FIAP_Ext_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning (Intro)"
      ],
      "metadata": {
        "id": "nWr4s29geLud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement learning is a type of machine learning where an agent learns to make better decisions through trial and error. In this setup, the *agent* acts as the learner or decision maker, while the *environment* represents everything the agent interacts with. As the agent explores the environment, it observes the current *state*—a snapshot of the situation—and selects an *action* in response. The environment then returns a new state and provides a *reward*, which signals how good or bad the action was. Over time, the agent uses these rewards to improve its decision-making and discover strategies that lead to better outcomes. Think of it like training a robot to ride an elevator: every time it presses the right button and reaches the desired floor, it earns a reward and learns to repeat that success."
      ],
      "metadata": {
        "id": "KzLFFkUdSDCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1400/1*suRqGq0gWXQV2bcMGAhA_A.jpeg)"
      ],
      "metadata": {
        "id": "cCFG9FtIYp4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.state = 0\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = self.calc_reward(action)\n",
        "        self.state = self.state + reward\n",
        "        # Takes an action and returns (next_state, reward, done)\n",
        "        if self.state == 3:\n",
        "          return (self.state, reward, True)\n",
        "        else:\n",
        "          return (self.state, reward, False)\n",
        "\n",
        "    def calc_reward(self, action):\n",
        "        if action == \"R\":   # (R)ight\n",
        "            return 1        # Reward of 1\n",
        "        elif action == \"L\": # (L)eft\n",
        "            return -1       # Penalty of -1\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.actions = [\"L\", \"R\"]\n",
        "\n",
        "    def choose_action(self):\n",
        "        return random.choice(self.actions)\n",
        ""
      ],
      "metadata": {
        "id": "6cQ_htQXSDH9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment()\n",
        "agent = Agent()\n",
        "\n",
        "done = False\n",
        "steps = 0\n",
        "\n",
        "while not done and steps < 30:\n",
        "    state = env.get_state()\n",
        "    action = agent.choose_action()\n",
        "    next_state, reward, done = env.step(action)\n",
        "\n",
        "    print(f\"Step {steps}: State={state}, Action={action}, Next State={next_state}, Reward={reward}\")\n",
        "    steps += 1\n",
        "\n",
        "if done:\n",
        "    print(\"🎉 Agent reached the goal!\")\n",
        "else:\n",
        "    print(\"❌ Agent did not reach the goal in time.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU8bPwdcbbQk",
        "outputId": "e869692a-afa9-4518-b1e7-a3db171c1260"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: State=0, Action=L, Next State=-1, Reward=-1\n",
            "Step 1: State=-1, Action=R, Next State=0, Reward=1\n",
            "Step 2: State=0, Action=R, Next State=1, Reward=1\n",
            "Step 3: State=1, Action=L, Next State=0, Reward=-1\n",
            "Step 4: State=0, Action=L, Next State=-1, Reward=-1\n",
            "Step 5: State=-1, Action=L, Next State=-2, Reward=-1\n",
            "Step 6: State=-2, Action=R, Next State=-1, Reward=1\n",
            "Step 7: State=-1, Action=R, Next State=0, Reward=1\n",
            "Step 8: State=0, Action=R, Next State=1, Reward=1\n",
            "Step 9: State=1, Action=R, Next State=2, Reward=1\n",
            "Step 10: State=2, Action=R, Next State=3, Reward=1\n",
            "🎉 Agent reached the goal!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ativity: Elevator"
      ],
      "metadata": {
        "id": "yRTvNmB0d4tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Design a reinforcement learning solution where an agent simulates a person trying to reach a desired floor using an elevator. The user will input three values: the number of floors in the building, the starting floor, and the target floor. The agent can choose to move either “up” or “down,” and the episode continues until the agent arrives at the destination. Reaching the target floor yields a reward of +1, while all other movements result in a reward of 0. The agent should learn to navigate efficiently, avoiding invalid moves beyond the building limits, and use the environment’s feedback to reach the goal. Students are expected to implement an environment and agent class, define the decision logic, and simulate the episode with this setup."
      ],
      "metadata": {
        "id": "8y7d13Nld6w5"
      }
    }
  ]
}